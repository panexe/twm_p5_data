{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "# Praktikum 5 \n",
    "- Text und Webmining WS 22/23 \n",
    "- Prof. Dr. Markus D√∂hring, Lars Neumann \n",
    "\n",
    "Dieses Notebook stellt das f√ºnfte und letzte Praktikum der Lehrveranstaltung dar. Die Aufgaben sollen alle vor Ort bearbeitet und besprochen werden.\n",
    "\n",
    "Ziel des Praktikums ist es den Prozess der Analyse und Modelerstellung bei sequentiellen Daten am Beispiel darzustellen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "<a id=\"toc\"></a>\n",
    "## üìÑ Inhaltverzeichnis\n",
    "\n",
    "*In Jupyter Lab kann in der linken Seitenleiste ebenfalls ein Inhaltverzeichnis verwendet werden.*\n",
    "\n",
    "- [Setup](#setup) \n",
    "    - [Imports](#imports)\n",
    "    - [Utility Code](#util)\n",
    "- [Parameter](#parameter) \n",
    "- [Teil 1: IMDB Datensatz](#dataset) \n",
    "    - [Explorative Datenanalyse](#eda)\n",
    "- [Teil 2: Erstellen und Training von Modellen](#models) \n",
    "- [Klassische Ans√§tze](#classics) \n",
    "    - [Logistische Regression](#logistic_regression) \n",
    "    - [Decision Trees](#decision_tree) \n",
    "- [Deep Learning](#deep_learning) \n",
    "    - [Optimizer, Fehlerfunktion und Metriken](#optimizer) \n",
    "    - [RNNs mit Embeddings](#rnn)\n",
    "    - [RNNs GloVE Embeddings](#rnn_glove) \n",
    "    - [LSTMs](#lstm)\n",
    "    - [Transformer / Bert](#bert) \n",
    "\n",
    "## üìã Aufgaben \n",
    "\n",
    "#### **Aufgabe 1** Datenexploration und klassische Ans√§tze\n",
    "- [a. Mit dem Datensatz vertraut machen](#a1_a)  \n",
    "- [b. Feature-Extraction](#a1_b)  \n",
    "- [c. Accuracy als Metrik?](#a1_c)  \n",
    "- [d. Interpretation der klassischen Modelle](#a1_d) \n",
    "#### **Aufgabe 2** Deep Learning: RNN & LSTM\n",
    "- [a. Interpretation des RNN](#a2_a)  \n",
    "- [b. Fehlvorhersagen betrachten](#a2_b)  \n",
    "- [c. Interpretation RNN mit GloVe Embeddings](#a2_c) \n",
    "- [d. Interpretation LSTM](#a2_d) \n",
    "#### **Aufgabe 3** Deep Learning: BERT/Transformer\n",
    "- [a. Interpretation BERT](#a3_a)\n",
    "- [b. BertViz](#a3_b)   \n",
    "- [c. Fehlvorhersagen betrachten und vergleichen](#a3_c) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "<a id='setup'></a>\n",
    "# üõ†Ô∏è Setup \n",
    "\n",
    "\n",
    "Wenn dieses Notebook als *'normales' Jupyter Notebook* ausgef√ºhrt wird gehen Sie bitten in der Men√ºleiste oben links auf den Reiter 'File' und dann auf 'Trust this Notebook'.\\\n",
    "In dem Pop-up Fenster m√ºssen Sie dann nochmal best√§tigen, dass Sie diesem Notebook vertrauen. Das ist notwendig um einige ineraktive Features dieses Notebooks verwenden zu k√∂nnen. \n",
    "\n",
    "In *Jupyter Lab* ist dies nicht notwendig. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Javascript, clear_output\n",
    "\n",
    "!apt-get update\n",
    "!apt install graphviz --fix-missing -y\n",
    "\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install -r requirements.txt\n",
    "!{sys.executable} -m pip install jupyter_black bertviz transformers datasets seaborn scikit-learn pydotplus tqdm matplotlib\n",
    "\n",
    "clear_output()\n",
    "print(\"Installation complete!\")\n",
    "\n",
    "# In Jupyter Notebook the kernel can be restarted via javascript\n",
    "# for Jupyter Lab the following line should be commented out\n",
    "Javascript(\"alert('Bitte Kernel neustarten!')\")\n",
    "# Javascript(\"Jupyter.notebook.restart_kernel();\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ùó  **Neustart** \\\n",
    "> Sobald die Installation abgeschlossen ist:\n",
    "> 1. Gehen sie bitte oben in der Taskleiste auf 'Kernel'\n",
    "> 2. Klicken auf 'Restart Kernel'\n",
    ">\n",
    "> Danach muss die vorangehende Zelle nicht erneut ausgef√ºhrt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "<a id=\"imports\"></a>\n",
    "### Imports\n",
    "\n",
    "üîù [Zur√ºck zum Anfang](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(lab=True)\n",
    "\n",
    "# vis\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets \n",
    "from IPython.display import (\n",
    "    Markdown,\n",
    "    HTML,\n",
    "    display_markdown,\n",
    "    display_html,\n",
    "    display,\n",
    "    Image,\n",
    "    Javascript,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "# i/o and system\n",
    "import gc \n",
    "import subprocess\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "from io import StringIO\n",
    "import pickle\n",
    "import joblib  # pickle for pipelines\n",
    "\n",
    "# typing\n",
    "from typing import Dict, Union, Any\n",
    "from typing import List, Tuple\n",
    "\n",
    "# deep learning\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# import RNN\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, LSTM\n",
    "\n",
    "# classic models\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# huggingface\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# custom utility functions / classes import \n",
    "from p5_util import * \n",
    "\n",
    "\n",
    "assert (\n",
    "    len(tf.config.list_physical_devices()) > 1\n",
    "), \"At least one cpu and one gpu should be available\"\n",
    "\n",
    "\n",
    "# checking reproducability / determinism \n",
    "# for each training the optimizer has the be initiliazed \n",
    "# otherwise the training isn't deterministic\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "# alternative: \n",
    "#os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "assert tf.random.uniform([1]).numpy() == 0.6645621, \"\"\"\n",
    "The tensorflow seeded random number doesn't match, reproducability might not be given!\"\"\"\n",
    "assert np.random.uniform() == 0.3745401188473625, \"\"\"\n",
    "The numpy seeded random number doesn't match, reproducability might not be given!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "<a id=\"util\"></a>\n",
    "## Utility Code \n",
    "\n",
    "> In diesem Notebook werden einige Hilfsfunktionen aus der Datei `p5_util.py` genutzt. \n",
    "> Es ist nicht notwendig f√ºr die Durchf√ºhrung dieses Praktikums, wer sich trotzdem einige dieser Funktionen anschauen m√∂chte kann dies gerne √ºber den Jupyter Dateiexplorer tun *(alternativ k√∂nnen sie [hier klicken](p5_util.py) um den Code zu √∂ffnen)*.\n",
    "> \n",
    "> Allgemein kann f√ºr jede Funktion mit `Shift` + `Tab` die in einer Funktion enthaltene Dokumentation angezeigt werden. Dies kann z.B. auch bei der Verwendung von Funktionen aus NumPy und anderen Bibliotheken hilfreich sein.   \n",
    "> \n",
    "> \n",
    "\n",
    "<a id='parameter'></a>\n",
    "# ‚öôÔ∏è Parameter\n",
    "- **VOCAB_SIZE:** Gibt die Anzahl an unqiue W√∂rtern im Datensatz an. Dabei werden die $n$ W√∂rter behalten, die am h√§ufigsten im Trainingsdatensatz vorkommen. Die weiteren W√∂rter werden durch einen oov_char (<u>o</u>ut <u>o</u>f <u>v</u>ocabulary) ersetzt. Dieser wird oft auch als \"UNK\" oder \"\\<unk>\" (f√ºr unknown) betitelt.\n",
    "- **SEQ_LEN:** Gibt die Anzahl an W√∂rtern an, auf die jede Input-Sequenz in der Vektordarstellung erweitert (padding) oder gek√ºrzt wird, um einen einheitlichen Input f√ºr das Modell zu erhalten. Beim padding wird ein spezielles Zeichen eingef√ºgt, das dem Model signalisiert soll, welche Bedeutung diese Zeichen haben. In der Integer-Darstellung werden diese Zeichen im folgenden durch 0 repr√§sentiert (manchmal auch \"PAD\" oder \"\\<pad>\" genannt). *Anmerkung: Nicht jedes Model ben√∂tigt dieses Datenformat.*\n",
    "- **CACHED**: Wenn True, werden viele Teile des Codes nicht neu ausgef√ºhrt, sondern aus Ergebnisdateien geladen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 250\n",
    "VOCAB_SIZE = 10000\n",
    "CACHED = False\n",
    "\n",
    "\n",
    "def set_values_callback(seq_dropdown, vocab_dropdown, cached_checkbox):\n",
    "    def fn(event):\n",
    "        global SEQ_LEN\n",
    "        global VOCAB_SIZE\n",
    "        global CACHED\n",
    "        SEQ_LEN = int(seq_dropdown.value)\n",
    "        VOCAB_SIZE = int(vocab_dropdown.value)\n",
    "        CACHED = cached_checkbox.value\n",
    "        display(\n",
    "            Javascript(\n",
    "                \"Jupyter.notebook.scroll_to_cell(Jupyter.notebook.get_selected_index()+1, 250)\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "create_set_value_ui(\n",
    "    default_vocab_size=10_000,\n",
    "    default_seq_len=250,\n",
    "    default_cached=False,\n",
    "    callback_fn=set_values_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "source": [
    "<a id='dataset'></a>\n",
    "# üóÉÔ∏è IMDB Datensatz\n",
    "\n",
    "Im folgenden wird der IMDB Movie Review Datensatz <sup>[1]</sup> geladen und f√ºr die Verwendung im Training von Modellen vorverarbeitet. Der Datensatz enth√§lt jeweils 25.000 Retensionen als Trainings- und Testdaten. \n",
    "\n",
    "Um den Datensatz zu laden wird die Tensorflow Keras API verwendet. Diese gibt den Datensatz in einer Integer Repr√§sentation aus.<br/>\n",
    "Das hei√üt ein Review kann z.B. so aussehen: $[23, 128, 3, 8, ...]$. <br/>\n",
    "Die API liefert au√üerdem einen word-index, der angibt welches Wort durch welche Zahl repr√§sentiert wird. Um Modelle zu trainieren, die einen String-Input ben√∂tigen, m√ºssen die Integer-Vektoren zuerst umgewandelt werden. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-block\"> \n",
    "[1]: Maas, Andrew L., Raymond E., Daly, Peter T., Pham, Dan, Huang, Andrew Y., Ng, and Christopher, Potts. \"Learning Word Vectors for Sentiment Analysis.\" . In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (pp. 142‚Äì150). Association for Computational Linguistics, 2011.\n",
    "<br/>   \n",
    "<a href=\"https://ai.stanford.edu/%7Eamaas/data/sentiment/\">Link zum Datensatz</a>\n",
    "</div>\n",
    "\n",
    "üîù *[Zur√ºck zum Anfang](#toc)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "if CACHED:\n",
    "    dataset = joblib.load(\"imdb_dataset.pkl\")\n",
    "    print_md(\"Loaded IMDb dataset.\")\n",
    "    (X_train, y_train), (X_test, y_test) = dataset.get_data()\n",
    "    (X_train_decoded, _), (X_test_decoded, _) = dataset.get_data(True)\n",
    "else:\n",
    "    dataset = IMDbDataset(vocabulary_size=VOCAB_SIZE)\n",
    "    (X_train, y_train), (X_test, y_test) = dataset.get_data()\n",
    "    (X_train_decoded, _), (X_test_decoded, _) = dataset.get_data(True)\n",
    "\n",
    "    # uncomment to save dataset\n",
    "    # joblib.dump(dataset, \"imdb_dataset.pkl\")\n",
    "\n",
    "dataset.print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='a1_a'></a>\n",
    "### üìã Aufgabe 1 a.\n",
    "Machen Sie Sie sich mit dem konkreten Keras-IMDB Datensatz vertraut. Schauen Sie sich z.b. einige Review-Texte und deren Labels an. \\\n",
    "Stimmen Sie als Leser mit den Labels √ºberein? Warum bzw. warum nicht?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen sie hier notieren.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a1_b'></a>\n",
    "### üìã Aufgabe 1 b.\n",
    "Identifizieren Sie die relevantesten Features (W√∂rter) anhand des Chi<sup>2</sup>-Wertes auf den Trainingsdaten. \n",
    "\n",
    "\n",
    "**üí° Tipp:** Nutzen Sie dazu aus sklearn: CountVectorizer und chi2, sowie aus numpy argsort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a1_c'></a>\n",
    "### üìã Aufgabe 1 c.\n",
    "Pr√ºfen und begr√ºnden Sie, ob ‚Äûaccuracy‚Äú ein sinnvolles G√ºtema√ü f√ºr einen Klassifikator auf diesem Datensatz ist.\n",
    "\n",
    "$\n",
    "ACC = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='models'></a>\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è Erstellen und Training von Modellen\n",
    "\n",
    "Es werden die folgenden Modelle auf den Datensatz angewendet: \n",
    "\n",
    "- [Logistische Regression](#log_regression)\n",
    "- [Classification Tree](#decision_tree) \n",
    "- [RNN](#rnn) \n",
    "- [RNN mit GloVe](#rnn_glove)\n",
    "- [LSTM](#lstm)\n",
    "- [Transformer / Bert](#bert)\n",
    "\n",
    "*[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "<a id='classics'></a>\n",
    "## Klassische Ans√§tze\n",
    "\n",
    "Zuerst werden klassische Machine Learning Modelle auf den Datensatz angewendet. Es geh√∂rt zu den Best Practices zuerst ein oder mehrere Baseline Modelle zum Vergleichen zu verwenden, um die Leistung der neuen Modelle in einen Kontext zu setzten bzw. zu evaluieren ob diese √ºberhaupt bessere Ergebnisse liefern.  \n",
    "\n",
    "<a id='log_regression'></a>\n",
    "### Logistische Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# option for this cell\n",
    "save_model = False\n",
    "\n",
    "if CACHED:\n",
    "    logistic_regression_model = joblib.load(\"logistic_regression_model.pkl\")\n",
    "    print_md(\"Loaded logistic_regression_model.pkl.\")\n",
    "else:\n",
    "    # eine Pipeline erlaubt es uns mehrere Module sequentiel zu einem Modell zusammenzuf√ºgen\n",
    "    logistic_regression_model = Pipeline(\n",
    "        [\n",
    "            (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2))),\n",
    "            (\"log\", LogisticRegression(random_state=RANDOM_STATE)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logistic_regression_model.fit(X_train_decoded, y_train)\n",
    "\n",
    "    if save_model:\n",
    "        joblib.dump(\n",
    "            logistic_regression_model, \"logistic_regression_model.pkl\", compress=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "\n",
    "\n",
    "# Ausgabe der Features sortiert nach ihren Koeffizienten innerhalb des LogReg Modells\n",
    "print(\"Relevanteste Koeffizienten: \")\n",
    "print(\n",
    "    np.array(logistic_regression_model.named_steps[\"tfidf\"].get_feature_names_out())[\n",
    "        np.argsort(logistic_regression_model.named_steps[\"log\"].coef_[0])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Vorhersagen f√ºr den Testdatensatz erzeugen\n",
    "y_pred_logistic_regression = logistic_regression_model.predict(X_test_decoded)\n",
    "\n",
    "# Metriken zur Analyse des Modells\n",
    "logistic_regression_report = classification_report(\n",
    "    y_pred_logistic_regression,\n",
    "    y_test,\n",
    "    target_names=dataset.CLASS_NAMES,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "# Metriken berechnen\n",
    "logistic_regression_accuracy = accuracy_score(y_test, y_pred_logistic_regression)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "visualize_classification_report(logistic_regression_report, dataset.CLASS_NAMES)\n",
    "print_md(\"$\\\\textbf{Accuracy=}\" + f\"{logistic_regression_accuracy}$\")\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_predictions:\n",
    "    joblib.dump(y_pred_logistic_regression, \"y_pred_logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='decision_tree'></a>\n",
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_model = False\n",
    "\n",
    "# Laden oder Trainieren des Modells\n",
    "if CACHED:\n",
    "    decision_tree_model = joblib.load(\"decision_tree_model.pkl\")\n",
    "    print_md(\"Loaded decision_tree_model.pkl.\")\n",
    "else:\n",
    "    # Sklearn Pipeline mit tf-idf und Decision Tree\n",
    "    decision_tree_model = Pipeline(\n",
    "        [\n",
    "            (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2))),\n",
    "            (\n",
    "                \"tree\",\n",
    "                DecisionTreeClassifier(\n",
    "                    criterion=\"gini\", max_depth=4, random_state=RANDOM_STATE\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Training des Modells\n",
    "    decision_tree_model.fit(X_train_decoded, y_train)\n",
    "\n",
    "    # Speichern (im Praktikum nicht relevant)\n",
    "    if save_model:\n",
    "        joblib.dump(decision_tree_model, \"decision_tree_model.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "\n",
    "# Vorhersagen f√ºr den Testdatensatz erzeugen\n",
    "y_pred_decision_tree = decision_tree_model.predict(X_test_decoded)\n",
    "\n",
    "# Metriken zur Analyse des Modells\n",
    "decision_tree_report = classification_report(\n",
    "    y_pred_decision_tree, y_test, target_names=dataset.CLASS_NAMES, output_dict=True\n",
    ")\n",
    "decision_tree_accuracy = accuracy_score(y_test, y_pred_decision_tree)\n",
    "visualize_classification_report(decision_tree_report, dataset.CLASS_NAMES)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "print_md(\"$\\\\textbf{Accuracy=}\" + f\"{decision_tree_accuracy}$\")\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_predictions:\n",
    "    joblib.dump(y_pred_decision_tree, \"y_pred_decision_tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Holt die Split-Tokens des Models\n",
    "list_decision_tree_split_arguments(decision_tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "if CACHED:\n",
    "    display(Image(filename=\"decision_tree_graph.png\"))\n",
    "else:\n",
    "    # Falls Sie https://www.graphviz.org/ installiert haben, koennen Sie den Decision Tree auch plotten\n",
    "    # In der twm VM koennen Sie graphviz mit: sudo apt-get -y install graphviz\n",
    "    # installieren\n",
    "    if check_graphviz_installation():\n",
    "        display_decision_tree(decision_tree_model)\n",
    "    else:\n",
    "        print(\"Please install graphviz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a1_d'></a>\n",
    "### üìã Aufgabe 1 d.\n",
    "\n",
    "Wie interpretieren Sie die Ergebnisse der ‚Äûeinfacheren‚Äú Klassifikationsmodelle auf den Testdaten bzw. was f√§llt Ihnen auf? \\\n",
    "Inwiefern deckt sich die Wichtigkeit der Features mit der, die Sie in Teilaufgabe b ermittelt haben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='deep_learning'></a>\n",
    "# üß† Deep Learning\n",
    "\n",
    "üîù *[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "### Anpassung der Trainingsdaten \n",
    "Die Inputs m√ºssen gepaddet werden damit alle Datenpunkte eine einheitliche L√§nge haben.<br> \n",
    "Das hei√üt, dass ein k√ºrzerer Text (in der Vektor-Repr√§sentation) mit Nullen aufgef√ºllt wird um eine bestimmte L√§nge zu erreichen. \n",
    "Ein L√§ngerer Text wird abgeschnitten.\n",
    "> z.B. Sequence-Length = 10: $[1,2,3,4,5,6]$ => $[1,2,3,4,5,6,0,0,0,0]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Padding auf die oben angegebene Sequenz-L√§nge\n",
    "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=SEQ_LEN)\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=SEQ_LEN)\n",
    "\n",
    "# Ausgabe der shapes\n",
    "print_md(\n",
    "    f\"\"\"\n",
    "$Pad \\\\space sequences \\\\space = \\\\space (samples \\\\times timesteps)$\n",
    "- $Padded \\\\space shape \\\\space (train) = {X_train_padded.shape}$\n",
    "- $Padded \\\\space shape \\\\space (test) = {X_test_padded.shape}$\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optimizer\"></a>\n",
    "### Optimizer, Fehlerfunktion und Metriken\n",
    "\n",
    "Um ein Deep Learning Modell zu trainieren sind, sind so wie bei den vorangehenden klassischen Machine Learning Modellen, eine Optimierungsfunktion und eine Fehlerfunktion notwendig. Da diese beim Deep Learning h√§ufig stark von der Architektur des Netzes und der Problemstellung abh√§ngig sind, m√ºssen diese mit h√∂herer Wahrscheinlichkeit als bei z.B. einem Decision Tree angepasst werden. \n",
    "\n",
    "- **Adam:** Als Optimierungsalgorithmus wird Adam gew√§hlt. Dieser z√§hlt zu der Familie der adaptiven Optimierungsalgorithmen. Das bedeutet er kann die Schrittgr√∂√üe der Fehlerr√ºckf√ºhrung selbst√§ndig anpassen und erleichtert somit die Hyperparameterwahl. [Mehr dazu](https://arxiv.org/pdf/1412.6980.pdf)\n",
    "- **Binary Crossentropy:** Als Fehlerfunktion wird die Binary Crossentropy, also der Spezialfall der Kreuentropy im diskreten bei einer Ergenismenge der M√§chtigkeit 2 gew√§hlt. Dabei besteht die M√∂glichkeit ein Netz mit Sigmoid-Aktivierung am Ausgang zu verwenden um ide Vorhersagen auf das Intervall $[0,1]$ zu mappen. Alternativ kann das Netz darauf trainiert werden Vorhersagen im Bereich von $[-\\infty, \\infty]$ zu machen. In der tensorflow-Dokumentation wird empfohlen letztere Option zu w√§hlen. [Mehr dazu](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "\n",
    "Zus√§tzlich werden typischerweise noch Metriken definiert um bereits w√§hrend des Traingsprozesses die Performance des Models √ºberwachen zu k√∂nnen. Im folgenden wird die breits bekannte **Accuracy** verwendet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def create_compile_options():\n",
    "    \"\"\"Initializes the optimizer, loss, and metrics\n",
    "\n",
    "    This can be called to assure reproducability of one or more cells.\n",
    "    If more than one cell should be reproducable no other code should be executed inbetween.\n",
    "    \"\"\"\n",
    "    global adam_optimizer\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    global bce_loss\n",
    "    # from_logits=True bedeutet, dass ein Zahlenbereich von [-inf,inf] erwartet wird\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "    global binary_acc\n",
    "    # Die BinaryAccuracy ist gleichbedeutend mit der normalen Accuracy f√ºr bin√§re Klassifikationsprobleme\n",
    "    binary_acc = [tf.keras.metrics.BinaryAccuracy()]\n",
    "\n",
    "\n",
    "# Init.\n",
    "create_compile_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnn'></a>\n",
    "### Recurrent Neural Networks (RNNs) mit Embeddings\n",
    "\n",
    "üîù *[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "RNNs sind eine Kategorie an Neuronalen Netzen, die vor der Tranformer-Architektur den Standard f√ºr die Verarbeitung von sequentiellen Daten dargestellt haben. Vereinfach kann man sich ein RNN als ein neuronales Netz vorstellen, das wie eine `for`-Schleife √ºber die Elemente einer Sequenz iteriert. \n",
    "Diese Art der Verarbeitung erm√∂glicht es z.B. den Zusammenhang zwischen W√∂rtern an unterschiedlichen Stellen im Satz zu erkennen.\n",
    "\n",
    "Das zentrale Problem, das alle RNN-Architekturen gemeinsam haben ist, das durch die `for`-Schleifen Verarbeitung keine Parallelisierung m√∂glich ist und somit die Berechnung dieser Modelle Zeitintensiv ist. Weiterhin hat diese Architektur Probleme damit Zusammenh√§nge √ºber l√§ngere Sequenzen zu verstehen, da mit l√§ngerer Sequenzl√§nge auch die Komplexit√§t der Informationen, die zu vorherigen Elementen der Sequenz behalten werden m√ºssen, steigt. \n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "Im Folgenden wird die die Keras-Klasse `Sequential` zu Modelerstellung verwendet. Die Klasse rep√§sentiert eine lineare Abfolge von Schichten (d.h. jede Schicht hat max. 1 Vorg√§nger bzw. Nachfolger). <br>\n",
    "Bei komplexeren Modellen kann z.B. ein Graph mit parallelen Schichten erstellt werden (d.h. eine Schicht hat potentiell mehr als einen Vorg√§nger bzw. Nachfolger). In einem solchen Fall muss ein anderes Format gew√§hlt werden um das Modell zu erstellen. \n",
    "\n",
    "Die verwendeten Schichten: \n",
    "- **Embedding:** Mapt ein als Integer dargestelltes Wort auf einen Feature-Vektor \n",
    "- **SimpleRNN:** Eine einfach sequentielle Variante der Fully Connected Schichten, die aus der Vorlesung bekannt sind. \n",
    "- **Dense:** Eine \"standard\" Fully Connected Schicht, die verwendet wird um den Output der RNN Schicht auf eine einzige Zahl zu reduzieren, die zur Klassifikation interpretiert werden kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def create_rnn(\n",
    "    vocab_size: int,\n",
    "    optimizer: [str, tf.keras.optimizers.Optimizer],\n",
    "    loss: Union[str, tf.keras.losses.Loss],\n",
    "    metrics: List[Union[str, tf.keras.metrics.Metric]],\n",
    "    rnn_units: int = 32,\n",
    "    embedding_dim: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and compiles a RNN model\n",
    "\n",
    "    Arguments:\n",
    "        vocab_size (int): number of words in vocabulary\n",
    "        optimizer (str | tf.keras.optimizers.Optimize): Optmizer used to compile the model\n",
    "        loss (str | tf.keras.losses.Loss):\n",
    "        metrics (List[str | tf.keras.metrics.Metric]):\n",
    "\n",
    "    Creates a sequential RNN with the following architecture:\n",
    "        - Embedding layer (vocab_size, embedding_dim)\n",
    "        - SimpleRNN layer\n",
    "        - Single neuron dense layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Ein Sequential Model ist eine einfache Folge von Schichten ohne Branches\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # Embedding Layer: mappt Index eines Tokens auf einen Dense-Vektor\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "\n",
    "    # Eine einfache Feed-Forward RNN Schicht\n",
    "    model.add(tf.keras.layers.SimpleRNN(units=rnn_units))\n",
    "\n",
    "    # Eine Dense-Schicht mit einem Neuron ohne Aktivierungsfunktion\n",
    "    model.add(Dense(1, activation=None))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# options for this cell\n",
    "save_history = (\n",
    "    False  # should not be needed as the same prediction vector is provided anyway\n",
    ")\n",
    "\n",
    "# assure reproducability\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# train or load trained model\n",
    "if CACHED:\n",
    "    rnn_model = tf.keras.models.load_model(\"rnn_model_e3.h5\")\n",
    "    rnn_training_history = joblib.load(\"rnn_training_history.pkl\")\n",
    "    print_md(\"Loaded rnn model and training-history.\")\n",
    "else:\n",
    "    # set for reproducablility\n",
    "    tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "    rnn_model = create_rnn(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        optimizer=adam_optimizer,\n",
    "        loss=bce_loss,\n",
    "        metrics=binary_acc,\n",
    "    )\n",
    "    rnn_training_history = rnn_model.fit(\n",
    "        X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2\n",
    "    )\n",
    "\n",
    "rnn_model.summary()\n",
    "\n",
    "if save_history:\n",
    "    joblib.dump(rnn_training_history, \"rnn_training_history.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_training_history(rnn_training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a2_a'></a>\n",
    "### üìã Aufgabe 2 a.\n",
    "Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten zum RNN bzw. was f√§llt Ihnen auf?\n",
    "\n",
    "Im folgenden wird das Model erneut auf den ganten Datensatz trainiert. <br/>\n",
    "Setzten sie den angzeigten Slider auf eine angemessene Anzahl an Epochen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "rnn_epoch_interacton = interactive(lambda epochs: epochs, epochs=(1, 10, 1), value=2)\n",
    "rnn_epoch_interacton.children[0].value = 2\n",
    "display(rnn_epoch_interacton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "save_model = False\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Das Model f√ºr das Training auf dem ganzen Datensatz neu erstellen\n",
    "rnn_model = create_rnn(VOCAB_SIZE, adam_optimizer, bce_loss, binary_acc)\n",
    "\n",
    "print_md(\"**Training des Models**\")\n",
    "rnn_model.fit(\n",
    "    X_train_padded, y_train, epochs=rnn_epoch_interacton.result, batch_size=128\n",
    ")\n",
    "\n",
    "print_md(\"**Evaluierung des Models auf den Testdaten**\")\n",
    "# Vorhersagen f√ºr den Testdatensatz erzeugen\n",
    "y_pred_logits_rnn_model = rnn_model.predict(X_test_padded)\n",
    "# Aktivierungsfunktion auf Logits anwenden (mapping [-inf, inf] -> [0,1])\n",
    "y_pred_rnn_model = (\n",
    "    tf.keras.activations.sigmoid(y_pred_logits_rnn_model).numpy().reshape(-1)\n",
    ")\n",
    "\n",
    "# Metriken zur Analyse des Modells\n",
    "rnn_model_accuracy = binary_acc[0](y_test, y_pred_rnn_model)\n",
    "rnn_model_loss = bce_loss(y_test.reshape(-1, 1), y_pred_logits_rnn_model)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "display_table(\n",
    "    table_data=[\n",
    "        [\"<strong>Accuracy auf den Testdaten</strong>\", \"%.4f\" % rnn_model_accuracy],\n",
    "        [f\"<strong>Loss auf den Testdaten</strong>\", \"%.4f\" % rnn_model_loss],\n",
    "    ],\n",
    "    use_header=False,\n",
    ")\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_predictions:\n",
    "    joblib.dump(y_pred_rnn_model.numpy(), \"y_pred_rnn_model.pkl\")\n",
    "if save_model:\n",
    "    rnn_model.save(\"rnn_model_e3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a2_b'></a>\n",
    "### üìã Aufgabe 2 b.\n",
    "\n",
    "Schauen Sie sich zum RNN einige der \"drastischsten\" FNs an (hoher Score und Label=1). K√∂nnen Sie erahnen, was das Modell ggf. verwirrt hat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# IHR CODE HIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen sie hier notieren*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rnn_glove'></a>\n",
    "### Recurrent Neural Networks mit GloVe Embeddings(RNNs)\n",
    "Im Folgenden wird die gleiche RNN-Architektur mit 50-dimensionalen GloVe Embeddings erstellt. Es wird der gleiche Optimizer, die gleiche Fehlerfunktion und die gleiche Metrik wie bei dem normalen RNN verwendet. Es werden potentiell nicht alle Best-Practices verwendet, die es zu Transfer-Learning gibt (Bsp. [Tensorflow Warm-Start Embeddings](https://www.tensorflow.org/tutorials/text/warmstart_embedding_matrix)).\n",
    "\n",
    "*[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "#### GloVe Embeddings laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Download & laden (sollte bereits im S3 Bucket abgelegt sein)\n",
    "download_glove_embeddings()\n",
    "glove_word_embeddings = load_glove_embeddings()\n",
    "\n",
    "# Es wird eine Embedding-Matrix mit den vorhandenen Tokens erstellt\n",
    "# nicht gefundende Tokens werden mit 0 initialisiert\n",
    "glove_weights = create_embeddings_matrix(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    word_index=dataset.word_index,\n",
    "    word_embeddings=glove_word_embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "type(glove_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def create_rnn_model_with_glove(\n",
    "    glove_weights: [np.ndarray],\n",
    "    optimizer: [str, tf.keras.optimizers.Optimizer],\n",
    "    loss: Union[str, tf.keras.losses.Loss],\n",
    "    metrics: List[Union[str, tf.keras.metrics.Metric]],\n",
    "    rnn_units: int = 32,\n",
    "    embedding_dim: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and compiles a RNN model with a given embedding matrix.\n",
    "\n",
    "    Arguments:\n",
    "        glove_weights (np.ndarray): weights for the embedding layer\n",
    "        vocab_size (int): number of words in vocabulary\n",
    "        optimizer (str | tf.keras.optimizers.Optimize): Optmizer used to compile the model\n",
    "        loss (str | tf.keras.losses.Loss):\n",
    "        metrics (List[str | tf.keras.metrics.Metric]):\n",
    "\n",
    "    Creates a sequential RNN with the following architecture:\n",
    "        - Embedding layer (vocab_size, embedding_dim)\n",
    "        - SimpleRNN layer\n",
    "        - Single neuron dense layer\n",
    "    \"\"\"\n",
    "    # Ein Sequential Model ist eine einfache Folge von Schichten ohne Branches\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=VOCAB_SIZE,\n",
    "            output_dim=50,\n",
    "            weights=[glove_weights],\n",
    "            trainable=False,\n",
    "        )\n",
    "    )\n",
    "    # Eine einfache Feed-Forward RNN Schicht\n",
    "    model.add(tf.keras.layers.SimpleRNN(units=rnn_units))\n",
    "\n",
    "    # Eine Dense-Schicht mit einem Neuron ohne Aktivierungsfunktion\n",
    "    model.add(Dense(1, activation=None))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modell trainieren bzw. laden**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_history = False\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Laden oder Trainieren des Modells\n",
    "if CACHED:\n",
    "    rnn_glove_training_history = joblib.load(\"rnn_glove_training_history.pkl\")\n",
    "    print_md(\"Loaded rnn_glove training history.\")\n",
    "else:\n",
    "    # Model erstellen\n",
    "    rnn_glove_model = create_rnn_model_with_glove(\n",
    "        glove_weights=glove_weights,\n",
    "        optimizer=adam_optimizer,\n",
    "        loss=bce_loss,\n",
    "        metrics=binary_acc,\n",
    "    )\n",
    "    # Aufbau des Netzes anzeigen\n",
    "    rnn_glove_model.summary()\n",
    "\n",
    "    # Training des Modells\n",
    "    rnn_glove_training_history = rnn_glove_model.fit(\n",
    "        x=X_train_padded, y=y_train, epochs=10, batch_size=128, validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Speichern (im Praktikum nicht relevant)\n",
    "    if save_history:\n",
    "        joblib.dump(\n",
    "            rnn_glove_training_history, \"rnn_glove_training_history.pkl\", compress=1\n",
    "        )\n",
    "\n",
    "%matplotlib inline\n",
    "plot_training_history(rnn_glove_training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modell auf vollst√§ndige Trainingsdaten trainieren** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "save_model = False\n",
    "rnn_glove_epochs = 3\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Neues Model f√ºr Training auf den vollst√§ndigen Datensatz erstellen\n",
    "rnn_glove_model = create_rnn_model_with_glove(\n",
    "    glove_weights=glove_weights,\n",
    "    optimizer=adam_optimizer,\n",
    "    loss=bce_loss,\n",
    "    metrics=binary_acc,\n",
    ")\n",
    "\n",
    "print_md(\"**Training des Models**\")\n",
    "rnn_glove_model.fit(X_train_padded, y_train, epochs=rnn_glove_epochs, batch_size=128)\n",
    "\n",
    "print_md(\"**Evaluierung des Models auf den Testdaten**\")\n",
    "# Vorhersagen f√ºr den Testdatensatz erzeugen\n",
    "# model.evaluate(...) wird nicht verwendet, um die Vorhersagen weiter analysieren zu k√∂nnen\n",
    "y_pred_logits_rnn_glove_model = rnn_glove_model.predict(X_test_padded)\n",
    "y_pred_rnn_glove_model = tf.keras.activations.sigmoid(y_pred_logits_rnn_glove_model)\n",
    "\n",
    "# Metriken zur Analyse des Modells\n",
    "rnn_glove_model_accuracy = binary_acc[0](y_test, y_pred_rnn_glove_model)\n",
    "rnn_glove_model_loss = bce_loss(y_test.reshape(-1, 1), y_pred_logits_rnn_glove_model)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "display_table(\n",
    "    table_data=[\n",
    "        [\n",
    "            \"<strong>Accuracy auf den Testdaten</strong>\",\n",
    "            \"%.4f\" % rnn_glove_model_accuracy,\n",
    "        ],\n",
    "        [f\"<strong>Loss auf den Testdaten</strong>\", \"%.4f\" % rnn_glove_model_loss],\n",
    "    ],\n",
    "    use_header=False,\n",
    ")\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_model:\n",
    "    rnn_glove_model.save(\"rnn_glove_model_e3.h5\")\n",
    "if save_predictions:\n",
    "    joblib.dump(\n",
    "        np.array(y_pred_rnn_glove_model).reshape(-1),\n",
    "        \"rnn_glove_training_history.pkl\",\n",
    "        compress=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a2_c'></a>\n",
    "### üìã Aufgabe 2 c.\n",
    "\n",
    "Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten zum RNN mit vorgelernten Glove-Embedding bzw. was f√§llt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lstm\"></a>\n",
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "üîù *[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "**Funktion die eine Instanz des Models erstellt** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(\n",
    "    optimizer: [str, tf.keras.optimizers.Optimizer],\n",
    "    loss: Union[str, tf.keras.losses.Loss],\n",
    "    metrics: List[Union[str, tf.keras.metrics.Metric]],\n",
    "    rnn_units: int = 32,\n",
    "    embedding_dim: int = 32,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates and compiles a LSTM model.\n",
    "\n",
    "    Arguments:\n",
    "        optimizer (str | tf.keras.optimizers.Optimize): Optmizer used to compile the model\n",
    "        loss (str | tf.keras.losses.Loss): Loss function used to compile the model\n",
    "        metrics (List[str | tf.keras.metrics.Metric]): List of metrics used to compile the model\n",
    "        rnn_units (int): Number of LSTM units\n",
    "        embedding_dim (int): Amount of embeddings dimensions\n",
    "\n",
    "    Creates a sequential RNN with the following architecture:\n",
    "        - Embedding layer (vocab_size, embedding_dim)\n",
    "        - LSTM layer\n",
    "        - Single neuron dense layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Sequentielles Modell erstellen\n",
    "    model = Sequential()\n",
    "\n",
    "    # Embedding Schicht f√ºr mapping von Token-Index auf Dense-Vektor\n",
    "    model.add(Embedding(VOCAB_SIZE, embedding_dim))\n",
    "\n",
    "    # Feed-Forward LSTM Schicht\n",
    "    model.add(LSTM(rnn_units, name=\"lstm\"))\n",
    "\n",
    "    # Eine Dense-Schicht mit einem Neuron ohne Aktivierungsfunktion\n",
    "    model.add(Dense(1, activation=None))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training des Models** \\\n",
    "*Im cached-Modus wird die Trainings-Historie geladen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_history = False\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Laden oder Trainieren des Modells\n",
    "if CACHED:\n",
    "    lstm_training_history = joblib.load(\"lstm_training_history.pkl\")\n",
    "    print_md(\"Done loading lstm training-history.\")\n",
    "\n",
    "    # Plot der Trainingshistorie\n",
    "    plot_training_history(lstm_training_history)\n",
    "else:\n",
    "    # LSTM Modell erstellen und trainieren\n",
    "    lstm_model = create_lstm_model(\n",
    "        optimizer=adam_optimizer, loss=bce_loss, metrics=binary_acc\n",
    "    )\n",
    "\n",
    "    # Training des Modells\n",
    "    lstm_training_history = lstm_model.fit(\n",
    "        X_train_padded, y_train, epochs=10, batch_size=128, validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot der Trainingshistorie\n",
    "    plot_training_history(lstm_training_history)\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_history:\n",
    "    joblib.dump(lstm_training_history, \"lstm_training_history.pkl\", compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "save_model = False\n",
    "lstm_training_epochs = 3\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Neues LSTM Model erstellen\n",
    "lstm_model = create_lstm_model(\n",
    "    optimizer=adam_optimizer, loss=bce_loss, metrics=binary_acc\n",
    ")\n",
    "\n",
    "print_md(\"**Training des Models**\")\n",
    "lstm_model.fit(X_train_padded, y_train, epochs=lstm_training_epochs, batch_size=128)\n",
    "\n",
    "print_md(\"**Evaluierung des Models auf den Testdaten**\")\n",
    "\n",
    "# Vorhersagen f√ºr den Testdatensatz erzeugen\n",
    "y_pred_logits_lstm_model = lstm_model.predict(X_test_padded)\n",
    "y_pred_lstm_model = (\n",
    "    tf.keras.activations.sigmoid(y_pred_logits_lstm_model).numpy().reshape(-1)\n",
    ")\n",
    "\n",
    "# Metriken zur Analyse des Modells\n",
    "lstm_model_accuracy = binary_acc[0](y_test, y_pred_lstm_model)\n",
    "lstm_model_loss = bce_loss(y_test.reshape(-1, 1), y_pred_logits_lstm_model)\n",
    "\n",
    "# Ausgabe der Metriken\n",
    "display_table(\n",
    "    table_data=[\n",
    "        [\"Metriken auf den Testdaten\", \"Value\"],\n",
    "        [\"<strong>Accuracy</strong>\", \"%.4f\" % lstm_model_accuracy],\n",
    "        [f\"<strong>Loss\", \"%.4f\" % lstm_model_loss],\n",
    "    ],\n",
    "    use_header=True,\n",
    ")\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_model:\n",
    "    lstm_model.save(\"lstm_model_e3.h5\")\n",
    "\n",
    "if save_predictions:\n",
    "    joblib.dump(y_pred_lstm_model.numpy(), \"y_pred_lstm_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a2_d'></a>\n",
    "### üìã Aufgabe 2 d.\n",
    "\n",
    "Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten zum LSTM bzw. was f√§llt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bert\"></a>\n",
    "### BERT \n",
    "\n",
    "üîù *[Zur√ºck zum Anfang](#toc)*\n",
    "\n",
    "**Datensatz konvertieren und tokenizen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Daten zu einem huggingface dataset konvertieren, um sie zu tokenizen\n",
    "ds_train = Dataset.from_dict({\"text\": X_train_decoded, \"label\": y_train})\n",
    "ds_test = Dataset.from_dict({\"text\": X_test_decoded, \"label\": y_test})\n",
    "\n",
    "print(\"ds_train:\\n\", ds_train, \"\\n\")\n",
    "print(\"ds_test:\\n\", ds_test, \"\\n\")\n",
    "\n",
    "# Tokenizer erstellen und auf Datensatz erstellen\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Trainingsdaten tokenizen\n",
    "tokenized_train_data = tokenizer(\n",
    "    ds_train[\"text\"],\n",
    "    return_tensors=\"np\",\n",
    "    max_length=SEQ_LEN,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "# Testdaten tokenizen\n",
    "tokenized_test_data = tokenizer(\n",
    "    ds_test[\"text\"],\n",
    "    return_tensors=\"np\",\n",
    "    max_length=SEQ_LEN,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "\n",
    "# Konvertieren der labels (redundant)\n",
    "train_labels = np.array(ds_train[\"label\"])\n",
    "test_labels = np.array(ds_test[\"label\"])\n",
    "\n",
    "# Tensorflow dataset f√ºr das Training erstellen\n",
    "tf_ds_train = (\n",
    "    tf.data.Dataset.from_tensor_slices((dict(tokenized_train_data), train_labels))\n",
    "    .cache()\n",
    "    .shuffle(25000)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training bzw. Finetuning des BERT-Models\n",
    "\n",
    "Die Warnung die angibt, dass kein Loss also keine Fehlerfunktion angegeben wird, kann einfach ignoriert werden. Das Modell enth√§lt bereits eine Fehlerfunktion im Model-Gaphen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from bertviz import model_view, head_view\n",
    "import torch\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "bert_adam_lr = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erstellen des Models** \\\n",
    "*Im cached-Modus wird es  geladen.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "use_additional_loss_fn = True\n",
    "save_model = False  # should be true for reloading without attention outputs\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# add own loss function to the output\n",
    "bert_loss = bce_loss if use_additional_loss_fn else None\n",
    "\n",
    "# Laden oder Trainieren des Models\n",
    "if CACHED:\n",
    "    # Laden des Models ohne attention output um OOM-Fehlern vorzubeugen\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        \"./bert_model_e2\", num_labels=1, output_attentions=False\n",
    "    )\n",
    "    # Compile: Adam optimizer hier mit niedrigerer learning-rate\n",
    "    bert_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(bert_adam_lr),\n",
    "        loss=bert_loss,\n",
    "        metrics=binary_acc,\n",
    "    )\n",
    "    print_md(\"Bert Model geladen und kompiliert.\")\n",
    "else:\n",
    "    # Distilbert Model laden\n",
    "    bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=1, output_attentions=False\n",
    "    )\n",
    "\n",
    "    # Compile: Adam optimizer hier mit niedrigerer learning-rate\n",
    "    bert_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(bert_adam_lr),\n",
    "        loss=bert_loss,\n",
    "        metrics=binary_acc,\n",
    "    )\n",
    "\n",
    "    # Training des Models\n",
    "    bert_history = bert_model.fit(tf_ds_train, epochs=2)\n",
    "\n",
    "    # Speichern (im Praktikum nicht relevant)\n",
    "    if save_model:\n",
    "        bert_model.save_pretrained(\"bert_model_e2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluierung auf Testdatensatz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cell-Options\n",
    "save_predictions = False\n",
    "\n",
    "# Reproduzierbarkeit dieser Zelle sicherstellen\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "print_md(\"**Evaluierung des Models auf den Testdaten**\")\n",
    "if CACHED:\n",
    "    y_pred_bert_model = joblib.load(\"y_pred_bert_model.pkl\")\n",
    "else:\n",
    "    y_pred_bert_model_logits = bert_model.predict(dict(tokenized_test_data)).logits\n",
    "    y_pred_bert_model = (\n",
    "        tf.keras.activations.sigmoid(y_pred_bert_model_logits).numpy().reshape(-1)\n",
    "    )\n",
    "\n",
    "# Metriken zur Analyse des Models\n",
    "bert_model_accuracy = tf.keras.metrics.binary_accuracy(y_test, y_pred_bert_model)\n",
    "print_md(f'**Accuracy auf den Testdaten:** {\"%.4f\" % bert_model_accuracy}')\n",
    "\n",
    "# Speichern (im Praktikum nicht relevant)\n",
    "if save_predictions:\n",
    "    joblib.dump(\n",
    "        y_pred_bert_model,\n",
    "        \"y_pred_bert_model.pkl\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a3_a'></a>\n",
    "### üìã Aufgabe 3 a.\n",
    "\n",
    "Wie interpretieren Sie die Performance-Kurven und Ergebnisse auf den Testdaten mit BERT bzw. was f√§llt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laden des Models mit attention-output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [],
   "source": [
    "# reload model but with output_attention=True\n",
    "create_compile_options()\n",
    "tf.keras.utils.set_random_seed(RANDOM_STATE)\n",
    "\n",
    "# Model zur Visualisierung mit attention-output laden\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"./bert_model_e2\", num_labels=1, output_attentions=True\n",
    ")\n",
    "# Compile\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.experimental.Adam(bert_adam_lr), metrics=binary_acc\n",
    ")\n",
    "\n",
    "# load and compile vanilla model\n",
    "bert_model_untrained = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=1, output_attentions=True\n",
    ")\n",
    "bert_model_untrained.compile(\n",
    "    optimizer=tf.keras.optimizers.experimental.Adam(bert_adam_lr), metrics=binary_acc\n",
    ")\n",
    "\n",
    "\n",
    "def bert_predict_and_show_attention(\n",
    "    review: str, use_finetuned_model=True, attention_multiplier: int = 1\n",
    "):\n",
    "    \"\"\"Predicts the sentiment of the given string and uses bert-viz to show attentions.\"\"\"\n",
    "\n",
    "    # Tokenize ohne padding\n",
    "    inputs = tokenizer(\n",
    "        review,\n",
    "        return_tensors=\"np\",\n",
    "        max_length=SEQ_LEN,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Vorhersage berechnen\n",
    "    if use_finetuned_model:\n",
    "        outputs = bert_model(inputs)\n",
    "    else:\n",
    "        outputs = bert_model_untrained(inputs)\n",
    "    attention = outputs[-1]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(np.squeeze(inputs[\"input_ids\"]))\n",
    "\n",
    "    # Konvertierung zu torch-tensor um bert-viz zu nutzen\n",
    "    torch_attentions = []\n",
    "    for i in range(len(attention)):\n",
    "        torch_attentions.append(\n",
    "            torch.from_numpy(attention[i].numpy()) * attention_multiplier\n",
    "        )\n",
    "    torch_attentions_tuple = tuple(torch_attentions)\n",
    "\n",
    "    # bert-viz anzeigen\n",
    "    head_view(torch_attentions_tuple, tokens)\n",
    "\n",
    "    # Ausgabe der Prediction\n",
    "    print(\n",
    "        \"Predicted Positive Sentiment\"\n",
    "        if tf.keras.activations.sigmoid(outputs[0]).numpy() > 0.5\n",
    "        else \"Predicted Negative Sentiment\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a3_b'></a>\n",
    "### üìã Aufgabe 3 b.\n",
    "\n",
    "Schauen Sie sich in BertViz f√ºr den Satz ‚ÄûThis movie was well thought out and badly executed‚Äù die Attention-Visualisierung sowohl f√ºr das Originalmodell, als auch das \n",
    "finetuned-Modell an und probieren Sie ggf. auch andere Beispiele aus: \n",
    "1. Starten Sie mit Head X aus Layer 0. Welche Muster erkennt dieser Head wahrscheinlich? Unterscheiden sich Originalmodell und finetuned-Modell?\n",
    "1. Untersuchen Sie die Attentions f√ºr den [CLS] Token aus Head X, Layer 5. Welche Muster erkennt dieser Head wahrscheinlich? Unterscheiden sich Originalmodell und finetuned-Modell?\n",
    "1. Untersuchen Sie weitere Attention Heads und probieren Sie weitere Beispiele aus. K√∂nnen Sie weitere Attention-Patterns erkennen?\n",
    "\n",
    "> **üí° Tipp**\\\n",
    "> Nutzen Sie die Parameter am Anfang der n√§chsten Zelle um den Output f√ºr unterschiedliche S√§tze bzw. das finetuned oder original Model zu erhalten.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell-options\n",
    "sample_idx = 0\n",
    "attention_multiplier = 3\n",
    "use_finetuned_model = True\n",
    "\n",
    "# Beispiel-Texte\n",
    "sample_texts = [\n",
    "    \"\"\"This movie was well thought out and badly executed\"\"\",\n",
    "    \"\"\"I went to see this movie\"\"\",\n",
    "    \"\"\"Tony is a really good author and this leads to an excellent and fast movie, which is great but not bad and small. Would watch again!\"\"\",\n",
    "    \"\"\"The nice movie was played by mediocre actors, but some of them had a beautiful hat with ugly stickers.\"\"\",\n",
    "]\n",
    "\n",
    "# Bert-Viz und Prediction anzeigen\n",
    "bert_predict_and_show_attention(\n",
    "    review=sample_texts[sample_idx],\n",
    "    use_finetuned_model=use_finetuned_model,\n",
    "    attention_multiplier=attention_multiplier,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='a3_c'></a>\n",
    "### üìã Aufgabe 3 c.\n",
    "\n",
    "Schauen Sie sich die BERT-Vorhersagen f√ºr die Top-FNs aus [2b](#a2_b) an ‚Äì wo funktioniert das Modell besser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ihre Antwort k√∂nnen Sie hier notieren*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "> # Changelog \n",
    ">\n",
    "> ### v0.6 (14.01.23) \n",
    "> - re-added feature relevance for logistic regression\n",
    "> - suppress warnings for transformers\n",
    "> - remove length inspection EDA (no relevance for practical)\n",
    "> - moved comments to end\n",
    "> - collapsed code\n",
    "> - removed outputs\n",
    ">\n",
    "> ### v0.5 (13.01.23) \n",
    "> - fit task descriptions to the pdf file \n",
    "> - added js alert telling to restart the kernel after installation \n",
    "> - added emojys to main headers (inspired by huggingface)\n",
    "> - added option to use bertviz on finetuned or original model \n",
    ">\n",
    "> ### v0.4 (11.01.23)\n",
    "> - adapt to Juypter Lab \n",
    ">   - black init settings \n",
    ">   - manual restart neccessary after package install \n",
    "> - Model metric reduced to accuracy only\n",
    "> - parameterized embedding-dim in model-create functions\n",
    "> - print model-metrics as a table\n",
    "> - bert-viz put into function, so diffent texts can be called without reloading the model \n",
    "> - completed comments \n",
    "> - removed model-prediction comparison \n",
    "> - execution time wihtout install: ~5-6 min \n",
    ">\n",
    "> ### v0.3 (07.02.23)\n",
    "> - general cleanup \n",
    "> - added binary_crossentropy loss to bert model\n",
    ">\n",
    "> ### v0.2 (06.02.23)\n",
    "> - moved utility code to a seperate file \n",
    "> - Installing librarys from requirements.txt\n",
    "> - removed '!nvidia-smi', the assertion in the imports cell should suffice\n",
    "> - fixed length per sample plot to show words instead of chars\n",
    "> - model comparison uses standard-scaler instead of min-max-scaler\n",
    "> - bert-model is saved and reloaded to change wether the attention is returned\n",
    "> - all models saved with correct seed \n",
    ">\n",
    ">\n",
    "> ### v0.1 (02.01.23)\n",
    "> - Added random seed to tensorflow & numpy after importing them\n",
    "> - Added experimental function call: `tf.config.experimental.enable_op_determinism()`\n",
    "> - for determinic cells, each training-cell needs to init the optimizer, for this a function is provided that initalizes all compile-arguments on a global level, thus its still apparent to the students that the same options are being used\n",
    ">     - This function is called: `create_compile_options()`\n",
    "> - Added random_state argument to sklearn models \n",
    "> - Addded accuracy to sklearn models (which is equal to the average precision)\n",
    "> - removed set_seed function, using transformers.set_seed instead \n",
    "> - added misssing torch import \n",
    "> - removed example solutions \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
